<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>PLS Overview • snazzieR</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="PLS Overview">
</head>
<body>
<p>








  custom.css

  

  </p>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="default" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">snazzieR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html">Home</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/pls-overview.html">PLS Overview</a></li>
    <li><a class="dropdown-item" href="../articles/pls-nipals.html">NIPALS Algorithm</a></li>
    <li><a class="dropdown-item" href="../articles/pls-svd.html">SVD Algorithm</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>PLS Overview</h1>
            
      

      <div class="d-none name"><code>pls-overview.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview-of-partial-least-squares-pls-regression">Overview of Partial Least Squares (PLS) Regression<a class="anchor" aria-label="anchor" href="#overview-of-partial-least-squares-pls-regression"></a>
</h2>
<p>The <code>snazzieR</code> package implements Partial Least Squares
regression with support for:</p>
<ul>
<li>The <strong>NIPALS</strong> algorithm</li>
<li>The <strong>SVD-based</strong> method</li>
<li>Beautiful <strong>console and LaTeX output</strong> formatting</li>
</ul>
<div class="section level3">
<h3 id="what-is-pls">What is PLS?<a class="anchor" aria-label="anchor" href="#what-is-pls"></a>
</h3>
<p>PLS regression is a dimension reduction technique that projects
predictors to a lower-dimensional space that maximally explains
covariance with the response variable(s). It is especially useful
when:</p>
<ul>
<li>Predictors are highly collinear</li>
<li>The number of predictors exceeds the number of observations</li>
</ul>
</div>
<div class="section level3">
<h3 id="learn-more">Learn More<a class="anchor" aria-label="anchor" href="#learn-more"></a>
</h3>
<ul>
<li><a href="pls-nipals.html">NIPALS Algorithm</a></li>
<li><a href="pls-svd.html">SVD Method</a></li>
<li><a href="pls-formatting.html">Output Formatting</a></li>
</ul>
</div>
<div class="section level3">
<h3 id="the-origins-of-pls">The Origins of PLS<a class="anchor" aria-label="anchor" href="#the-origins-of-pls"></a>
</h3>
<div style="text-indent: 2em;">
<p>The story of PLS begins in the snowy mountains of Uppsala, Sweden. In
the early 1960s, <strong>Herman Wold</strong> was working on a method to
analyze the relationship between a set of predictors and a set of
responses. He was particularly interested in the case where the
predictors were highly collinear, which made traditional regression
methods unreliable. Wold’s solution was to use a <em>latent variable
approach</em>, where he would extract a small number of latent variables
from the predictors and use them to predict the responses. This method
was later named <strong>Partial Least Squares (PLS)</strong>. The
initial algorithm was called <strong>“NILES”</strong> (Nonlinear
Iterative Least Squares), which was later renamed to
<strong>“NIPALS”</strong> (Nonlinear Iterative Partial Least
Squares).</p>
<p>Coming from a background in econometrics, where cleanly independent
predictors are rare, Wold was no stranger to the challenges introduced
by multicollinearity. In 1966, he laid the groundwork for what would
become PLS by introducing an iterative least squares method to estimate
principal components [11]. By 1969, he and colleagues applied these
methods to econometric problems including canonical correlation analysis
and fixed-point estimation [15].</p>
<p>PLS emerged as an extension of PCA. Both reduce dimensionality while
preserving structure, but PLS goes further by maximizing the covariance
between predictors and responses—making it a more effective regression
tool.</p>
</div>
<p align="center">
<img src="figures/Herman_Wold.jpg" width="30%" alt="Herman Wold"><br><small>Herman Wold; the father of PLS [10]</small>
</p>
<div style="text-indent: 2em;">
<p>In 1975, Wold and colleagues released a formal description of the PLS
algorithm with applications to increasingly large predictor spaces [13].
This laid the foundation for the so-called <strong>“Basic
Design”</strong> of PLS in the late 1970s [14], which provided a more
formal treatment of algorithm convergence.</p>
<p>By the 1980s, PLS gained traction in <strong>chemometrics</strong>,
particularly for spectroscopy and chromatography. The iterative
<strong>NIPALS</strong> algorithm was well-suited to handling large
variable sets and missing data [16]. Software packages helped adoption,
though theoretical work briefly stalled.</p>
<p>This changed in the 1990s with a resurgence in PLS development, aided
by modern computing power. A key moment came with <strong>Sijmen de
Jong’s 1993 publication</strong> of an <strong>SVD-based</strong>
approach to PLS [3], which computed all components at once instead of
iteratively. This dramatically improved efficiency and opened PLS to
large-scale applications such as <strong>genomics</strong>,
<strong>proteomics</strong>, and <strong>metabolomics</strong> [7].</p>
<p>Modern variants like <strong>sparse PLS</strong> and <strong>kernel
PLS</strong> emerged to address high-dimensional or nonlinear problems.
And with the rise of <strong>R</strong> and <strong>Python</strong>
ecosystems, community-driven implementations made PLS more accessible
than ever [8].</p>
</div>
<p align="center">
<img src="figures/timeline.png" width="100%" alt="Timeline of PLS Development"></p>
<div style="text-indent: 2em;">
<p>In the 2000s, PLS found application in <strong>neuroimaging</strong>
(e.g., fMRI analysis [5]) and later in <strong>machine learning</strong>
and <strong>data mining</strong> through kernel-based extensions [9]. As
toolkits matured, it became a staple for classification, regression, and
feature extraction.</p>
<p>Recent work integrates PLS with <strong>Bayesian models</strong>,
<strong>ensemble methods</strong>, and even <strong>deep
learning</strong>. For example, <em>Deep PLS</em> methods combine
interpretability with powerful representation learning [6]. With this
continued expansion, PLS remains one of the most flexible and powerful
tools for multivariate statistical analysis.</p>
</div>
<hr>
</div>
<div class="section level3">
<h3 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<ol>
<li>
Abdi, H. (2010). Partial least squares regression and projection on
latent structure regression (PLS regression). <em>Technical report</em>.
University of Texas at Dallas.
<a href="https://www.utdallas.edu/~herve/Abdi-PLS-pretty.pdf" class="external-link">https://www.utdallas.edu/~herve/Abdi-PLS-pretty.pdf</a>
</li>
<li>
Abdi, H., &amp; Williams, L. J. (2013). Partial least squares methods:
Partial least squares correlation and partial least square regression.
<em>Methods in Molecular Biology</em>, <em>930</em>, 549–579.
<a href="https://doi.org/10.1007/978-1-62703-059-5_23" class="external-link">doi.org/10.1007/978-1-62703-059-5_23</a>
</li>
<li>
de Jong, S. (1993). SIMPLS: An alternative approach to partial least
squares regression. <em>Chemometrics and Intelligent Laboratory
Systems</em>, <em>18</em>(3), 251–263.
<a href="https://doi.org/10.1016/0169-7439(93)85002-X" class="external-link">doi.org/10.1016/0169-7439(93)85002-X</a>
</li>
<li>
Ghosh, S., &amp; Doshi-Velez, F. (2017). Model selection in Bayesian
neural networks via horseshoe priors. <em>Journal of Machine Learning
Research</em>, <em>20</em>(1), 1–46.
</li>
<li>
Krishnan, A., Williams, L. J., McIntosh, A. R., &amp; Abdi, H. (2011).
Partial least squares (PLS) methods for neuroimaging: A tutorial and
review. <em>NeuroImage</em>, <em>56</em>(2), 455–475.
<a href="https://doi.org/10.1016/j.neuroimage.2010.07.034" class="external-link">doi.org/10.1016/j.neuroimage.2010.07.034</a>
</li>
<li>
Kong, X., &amp; Ge, Z. (2023). Deep PLS: A lightweight deep learning
model for interpretable and efficient data analytics. <em>IEEE
Transactions on Neural Networks and Learning Systems</em>,
<em>34</em>(11), 8923–8937.
<a href="https://doi.org/10.1109/TNNLS.2022.3154090" class="external-link">doi.org/10.1109/TNNLS.2022.3154090</a>
</li>
<li>
Nguyen, D. V., &amp; Rocke, D. M. (2002). Tumor classification by
partial least squares using microarray gene expression data.
<em>Bioinformatics</em>, <em>18</em>(1), 39–50.
<a href="https://doi.org/10.1093/bioinformatics/18.1.39" class="external-link">doi.org/10.1093/bioinformatics/18.1.39</a>
</li>
<li>
Rohart, F., Gautier, B., Singh, A., &amp; Lê Cao, K.-A. (2017).
mixOmics: An R package for ’omics feature selection and multiple data
integration. <em>PLOS Computational Biology</em>, <em>13</em>(11),
e1005752.
<a href="https://doi.org/10.1371/journal.pcbi.1005752" class="external-link">doi.org/10.1371/journal.pcbi.1005752</a>
</li>
<li>
Rosipal, R., &amp; Trejo, L. J. (2001). Kernel partial least squares
regression in reproducing kernel Hilbert space. <em>Journal of Machine
Learning Research</em>, <em>2</em>, 97–123.
</li>
<li>
Wikipedia contributors. (2025, March 22). Herman Wold.
<em>Wikipedia</em>.
<a href="https://en.wikipedia.org/wiki/Herman_Wold#/media/File:Professor_Herman_Wold,_Uppsala,_1969.jpg" class="external-link">Wikipedia
Link</a>
</li>
<li>
Wold, H. (1966). Estimation of principal components and related models
by iterative least squares. In P. R. Krishnajah (Ed.), <em>Multivariate
analysis</em> (pp. 391–420). Academic Press.
</li>
<li>
Wold, H. (1973). Nonlinear iterative partial least squares (NIPALS)
modelling: Some current developments. In P. R. Krishnaiah (Ed.),
<em>Multivariate Analysis III</em> (pp. 383–407). Academic Press.
</li>
<li>
Wold, H. (1975). Soft modelling by latent variables: The non-linear
iterative partial least squares (NIPALS) approach. In <em>Perspectives
in Probability and Statistics</em> (pp. 117–142). Academic Press.
</li>
<li>
Wold, H. (1979). Model construction and evaluation when theoretical
knowledge is scarce: The PLS approach to latent variables. In K. G.
Jöreskog &amp; H. Wold (Eds.), <em>Systems under indirect observation:
Causality, structure, prediction</em> (Vol. 2, pp. 47–74).
North-Holland.
</li>
<li>
Wold, H., &amp; Lyttkens, E. (1969). Nonlinear iterative partial least
squares (NIPALS) estimation procedures. <em>Bulletin of the
International Statistical Institute</em>, <em>43</em>, 29–51.
</li>
<li>
Wold, S., Sjöström, M., &amp; Eriksson, L. (1996). PLS-regression: A
basic tool of chemometrics. <em>Chemometrics and Intelligent Laboratory
Systems</em>, <em>58</em>(2), 109–130.
<a href="https://doi.org/10.1016/S0169-7439(01)00155-1" class="external-link">doi.org/10.1016/S0169-7439(01)00155-1</a>
</li>
<li>
Wright, K. (2017, October 27). The NIPALS algorithm.
<em>R-Project.org</em>.
<a href="https://cran.r-project.org/web/packages/nipals/vignettes/nipals_algorithm.html" class="external-link">https://cran.r-project.org/web/packages/nipals/vignettes/nipals_algorithm.html</a>
</li>
</ol>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Aidan J. Wagner.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  
</body>
</html>
